
@article{Bo_etal_2017,
  title = {An {{IRT}} Forecasting Model: Linking Proper Scoring Rules to Item Response Theory},
  shorttitle = {An {{IRT}} Forecasting Model},
  author = {Bo, Yuanchao Emily and Budescu, David V. and Lewis, Charles and Tetlock, Philip E. and Mellers, Barbara},
  date = {2017-03},
  journaltitle = {Judgment and Decision Making},
  volume = {12},
  number = {2},
  pages = {90--103},
  issn = {1930-2975},
  doi = {10.1017/S1930297500005647},
  url = {https://www.cambridge.org/core/journals/judgment-and-decision-making/article/an-irt-forecasting-model-linking-proper-scoring-rules-to-item-response-theory/AC002490811A4CC2A5FEB0FE43126538},
  urldate = {2025-02-17},
  abstract = {This article proposes an Item Response Theoretical (IRT) forecasting model that incorporates proper scoring rules and provides evaluations of forecasters’ expertise in relation to the features of the specific questions they answer. We illustrate the model using geopolitical forecasts obtained by the Good Judgment Project (GJP) (see Mellers, Ungar, Baron, Ramos, Gurcay, Fincher, Scott, Moore, Atanasov, Swift, Murray, Stone \& Tetlock, 2014). The expertise estimates from the IRT model, which take into account variation in the difficulty and discrimination power of the events, capture the underlying construct being measured and are highly correlated with the forecasters’ Brier scores. Furthermore, our expertise estimates based on the first three years of the GJP data are better predictors of both the forecasters’ fourth year Brier scores and their activity level than the overall Brier scores obtained and Merkle’s (2016) predictions, based on the same period. Lastly, we discuss the benefits of using event-characteristic information in forecasting.},
  langid = {english},
  keywords = {Brier scores,Forecasting,Gibbs sampling,Good Judgment Project,IRT,Proper Scoring Rules},
  file = {C:\Users\fabio\Zotero\storage\TZN94U7M\Bo et al. - 2017 - An IRT forecasting model linking proper scoring r.pdf}
}

@article{Feng_etal_2015,
  title = {Bayesian Quantile Regression with Approximate Likelihood},
  author = {Feng, Yang and Chen, Yuguo and He, Xuming},
  date = {2015-05},
  journaltitle = {Bernoulli},
  volume = {21},
  number = {2},
  pages = {832--850},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.3150/13-BEJ589},
  url = {https://projecteuclid.org/journals/bernoulli/volume-21/issue-2/Bayesian-quantile-regression-with-approximate-likelihood/10.3150/13-BEJ589.full},
  urldate = {2024-08-04},
  abstract = {Quantile regression is often used when a comprehensive relationship between a response variable and one or more explanatory variables is desired. The traditional frequentists’ approach to quantile regression has been well developed around asymptotic theories and efficient algorithms. However, not much work has been published under the Bayesian framework. One challenging problem for Bayesian quantile regression is that the full likelihood has no parametric forms. In this paper, we propose a Bayesian quantile regression method, the linearly interpolated density (LID) method, which uses a linear interpolation of the quantiles to approximate the likelihood. Unlike most of the existing methods that aim at tackling one quantile at a time, our proposed method estimates the joint posterior distribution of multiple quantiles, leading to higher global efficiency for all quantiles of interest. Markov chain Monte Carlo algorithms are developed to carry out the proposed method. We provide convergence results that justify both the algorithmic convergence and statistical approximations to an integrated-likelihood-based posterior. From the simulation results, we verify that LID has a clear advantage over other existing methods in estimating quantities that relate to two or more quantiles.},
  keywords = {Bayesian inference,linear interpolation,Markov chain Monte Carlo,Quantile regression},
  file = {C:\Users\fabio\Zotero\storage\857MRT4Z\Feng et al. - 2015 - Bayesian quantile regression with approximate like.pdf}
}

@online{Himmelstein_etal_2024a,
  title = {The {{Forecasting Proficiency Test}}: {{A General Use Assessment}} of {{Forecasting Ability}}},
  shorttitle = {The {{Forecasting Proficiency Test}}},
  author = {Himmelstein, Mark and Zhu, Sophie Ma and Petrov, Nikolay and Karger, Ezra and Helmer, Jessica and Livnat, Sivan and Bennett, Amory and Hedley, Page and Tetlock, Phil},
  date = {2024-11-18},
  eprinttype = {OSF},
  doi = {10.31234/osf.io/a7kdx},
  url = {https://osf.io/a7kdx_v1},
  urldate = {2025-02-18},
  abstract = {How can we identify accurate forecasters? Current gold standard approaches draw from two sources of information: simple cognitive assessments of reasoning ability and past forecasting accuracy. Cognitive assessments are well-studied and normed psychometric instruments, but have lower predictive value. Real-world forecasting problems are more challenging to norm a priori: they typically require a large comparison sample of forecasters to make predictive inferences. This paper develops the Forecasting Proficiency Test (FPT): a one-hour test that predicts more than 60\% of the variation in out-of-sample forecasting accuracy, double the predictive utility of prior work. We highlight several key results: (1) Eliciting forecasts in a quantile format is considerably more psychometrically reliable, meaning it is more efficient at reducing measurement error about forecasters' expected accuracy with fewer items, than forecasts elicited using the standard probability format common to most judgmental forecasting tournaments. (2) Several cognitive tasks, novel to forecasting research, are highly predictive of forecasting ability. These include denominator neglect, Bayesian updating, and adherence to decision rules. (3) Although our methods do best at separating out the worst forecasters from the population, they are also remarkably effective at identifying elite talent in the upper range of the skill distribution.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {/unread,Elicitation Methods,Forecasting,Superforecasters,Wisdom of Crowds},
  file = {C:\Users\fabio\Zotero\storage\D3TVWHJJ\Himmelstein et al. - 2024 - The Forecasting Proficiency Test A General Use As.pdf}
}



@article{Ferrando_2002,
  title = {Theoretical and {{Empirical Comparisons}} between {{Two Models}} for {{Continuous Item Response}}},
  author = {Ferrando, Pere J.},
  date = {2002-10-01},
  journaltitle = {Multivariate Behavioral Research},
  volume = {37},
  number = {4},
  eprint = {26816326},
  eprinttype = {pmid},
  pages = {521--542},
  publisher = {Routledge},
  issn = {0027-3171},
  doi = {10.1207/S15327906MBR3704_05},
  url = {https://doi.org/10.1207/S15327906MBR3704_05},
  urldate = {2024-09-08},
  abstract = {This article analyzes the relations between two continuous response models intended for typical response items: the linear congeneric model and Samejima's continuous response model (CRM). Using a factor analytical (FA) approach based on the assumption of underlying response variables, I describe how a particular case of the CRM can be considered as a nonlinear counterpart of Spearman's FA model. The mathematical relations between the: item-trait regressions, item parameter values, and conditional and marginal distributions of both models are obtained. The results allow (a) the item parameter values of the linear model to be obtained from CRM item parameter values, and (b) the conditions in which the congeneric model will be a good approximation to the CRM to be predicted. The relations described are illustrated using an empirical example and assessed by means of a simulation study.},
  keywords = {/unread}
}



@article{Grushka-Cockayne_etal_2017,
  title = {Quantile {{Evaluation}}, {{Sensitivity}} to {{Bracketing}}, and {{Sharing Business Payoffs}}},
  author = {Grushka-Cockayne, Yael and Lichtendahl, Kenneth C. and Jose, Victor Richmond R. and Winkler, Robert L.},
  date = {2017-06},
  journaltitle = {Operations Research},
  volume = {65},
  number = {3},
  pages = {712--728},
  publisher = {INFORMS},
  issn = {0030-364X},
  doi = {10.1287/opre.2017.1588},
  url = {https://pubsonline.informs.org/doi/10.1287/opre.2017.1588},
  urldate = {2025-02-17},
  abstract = {From forecasting competitions to conditional value-at-risk requirements, the use of multiple quantile assessments is growing in practice. To evaluate them, we use a rule from the general class of proper scoring rules for a forecaster’s multiple quantiles of a single uncertain quantity of interest. The general rule is additive in the component scores. Each component contains a function that measures its quantile’s distance from the realization and weights its contribution to the overall score. To determine this function, we propose that the score of a group’s combined quantile should be better than that of a randomly selected forecaster’s quantile only when the forecasters bracket the realization (i.e., their quantiles do not fall on the same side of the realization). If a score satisfies this property, we say it is sensitive to bracketing. We characterize the class of proper scoring rules that is sensitive to bracketing when the decision maker uses a generalized average to combine forecasters’ quantiles. Finally, we show how weights can be set to match the payoffs in many important business contexts.},
  keywords = {expert aggregation,expert combination,forecast evaluation,probability elicitation,proper scoring rules,quantile forecasts},
  file = {C:\Users\fabio\Zotero\storage\LQC7WLWT\Grushka-Cockayne et al. - 2017 - Quantile Evaluation, Sensitivity to Bracketing, an.pdf}
}

@article{Himmelstein_etal_2021,
  title = {Forecasting Forecaster Accuracy: {{Contributions}} of Past Performance and Individual Differences},
  shorttitle = {Forecasting Forecaster Accuracy},
  author = {Himmelstein, Mark and Atanasov, Pavel and Budescu, David V.},
  date = {2021-03},
  journaltitle = {Judgment and Decision Making},
  volume = {16},
  number = {2},
  pages = {323--362},
  issn = {1930-2975},
  doi = {10.1017/S1930297500008597},
  url = {https://www.cambridge.org/core/journals/judgment-and-decision-making/article/forecasting-forecaster-accuracy-contributions-of-past-performance-and-individual-differences/914F973B8C6A9CDCD3F1EA7CF17048D2},
  urldate = {2024-07-31},
  abstract = {A growing body of research indicates that forecasting skill is a unique and stable trait: forecasters with a track record of high accuracy tend to maintain this record. But how does one identify skilled forecasters effectively? We address this question using data collected during two seasons of a longitudinal geopolitical forecasting tournament. Our first analysis, which compares psychometric traits assessed prior to forecasting, indicates intelligence consistently predicts accuracy. Next, using methods adapted from classical test theory and item response theory, we model latent forecasting skill based on the forecasters’ past accuracy, while accounting for the timing of their forecasts relative to question resolution. Our results suggest these methods perform better at assessing forecasting skill than simpler methods employed by many previous studies. By parsing the data at different time points during the competitions, we assess the relative importance of each information source over time. When past performance information is limited, psychometric traits are useful predictors of future performance, but, as more information becomes available, past performance becomes the stronger predictor of future accuracy. Finally, we demonstrate the predictive validity of these results on out-of-sample data, and their utility in producing performance weights for wisdom-of-crowds aggregations.},
  langid = {english},
  keywords = {forecasting,hybrid forecasting competition,individual differences,item response models,longitudinal analysis,skill assessment,wisdom-of-crowds},
  file = {C:\Users\fabio\Zotero\storage\9MD3U5FN\Himmelstein et al. - 2021 - Forecasting forecaster accuracy Contributions of .pdf}
}

@online{Himmelstein_etal_2024,
  title = {The {{Forecasting Proficiency Test}}: {{A General Use Assessment}} of {{Forecasting Ability}}},
  shorttitle = {The {{Forecasting Proficiency Test}}},
  author = {Himmelstein, Mark and Zhu, Sophie Ma and Petrov, Nikolay and Karger, Ezra and Helmer, Jessica and Livnat, Sivan and Bennett, Amory and Hedley, Page and Tetlock, Phil},
  date = {2024-11-18},
  eprinttype = {OSF},
  doi = {10.31234/osf.io/a7kdx},
  url = {https://osf.io/a7kdx},
  urldate = {2024-12-16},
  abstract = {How can we identify accurate forecasters? Current gold standard approaches draw from two sources of information: simple cognitive assessments of reasoning ability and past forecasting accuracy. Cognitive assessments are well-studied and normed psychometric instruments, but have lower predictive value. Real-world forecasting problems are more challenging to norm a priori: they typically require a large comparison sample of forecasters to make predictive inferences. This paper develops the Forecasting Proficiency Test (FPT): a one-hour test that predicts more than 60\% of the variation in out-of-sample forecasting accuracy, double the predictive utility of prior work. We highlight several key results: (1) Eliciting forecasts in a quantile format is considerably more psychometrically reliable, meaning it is more efficient at reducing measurement error about forecasters' expected accuracy with fewer items, than forecasts elicited using the standard probability format common to most judgmental forecasting tournaments. (2) Several cognitive tasks, novel to forecasting research, are highly predictive of forecasting ability. These include denominator neglect, Bayesian updating, and adherence to decision rules. (3) Although our methods do best at separating out the worst forecasters from the population, they are also remarkably effective at identifying elite talent in the upper range of the skill distribution.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {Elicitation Methods,Forecasting,Superforecasters,Wisdom of Crowds},
  file = {C:\Users\fabio\Zotero\storage\7YMCR2WU\Himmelstein et al. - 2024 - The Forecasting Proficiency Test A General Use As.pdf}
}

@online{Ji_etal_2024,
  title = {Valid Standard Errors for {{Bayesian}} Quantile Regression with Clustered and Independent Data},
  author = {Ji, Feng and Lee, JoonHo and Rabe-Hesketh, Sophia},
  date = {2024-07-13},
  eprint = {2407.09772},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2407.09772},
  url = {http://arxiv.org/abs/2407.09772},
  urldate = {2024-08-04},
  abstract = {In Bayesian quantile regression, the most commonly used likelihood is the asymmetric Laplace (AL) likelihood. The reason for this choice is not that it is a plausible data-generating model but that the corresponding maximum likelihood estimator is identical to the classical estimator by Koenker and Bassett (1978), and in that sense, the AL likelihood can be thought of as a working likelihood. AL-based quantile regression has been shown to produce good finite-sample Bayesian point estimates and to be consistent. However, if the AL distribution does not correspond to the data-generating distribution, credible intervals based on posterior standard deviations can have poor coverage. Yang, Wang, and He (2016) proposed an adjustment to the posterior covariance matrix that produces asymptotically valid intervals. However, we show that this adjustment is sensitive to the choice of scale parameter for the AL likelihood and can lead to poor coverage when the sample size is small to moderate. We therefore propose using Infinitesimal Jackknife (IJ) standard errors (Giordano \& Broderick, 2023). These standard errors do not require resampling but can be obtained from a single MCMC run. We also propose a version of IJ standard errors for clustered data. Simulations and applications to real data show that the IJ standard errors have good frequentist properties, both for independent and clustered data. We provide an R-package that computes IJ standard errors for clustered or independent data after estimation with the brms wrapper in R for Stan.},
  pubstate = {prepublished},
  keywords = {/unread,Statistics - Applications,Statistics - Methodology},
  file = {C\:\\Users\\fabio\\Zotero\\storage\\3ZAK2AHL\\Ji et al. - 2024 - Valid standard errors for Bayesian quantile regres.pdf;C\:\\Users\\fabio\\Zotero\\storage\\T53RGMQD\\2407.html}
}

@article{Jose_Winkler_2009,
  title = {Evaluating {{Quantile Assessments}}},
  author = {Jose, Victor Richmond R. and Winkler, Robert L.},
  date = {2009},
  journaltitle = {Operations Research},
  volume = {57},
  number = {5},
  eprint = {25614838},
  eprinttype = {jstor},
  pages = {1287--1297},
  publisher = {INFORMS},
  issn = {0030-364X},
  url = {https://www.jstor.org/stable/25614838},
  urldate = {2025-02-17},
  abstract = {Quantile assessments are commonly encountered in the elicitation of probability distributions in decision analysis, forecasting, and risk analysis. Scoring rules have been developed to provide ex ante incentives for careful and truthful assessments and ex post evaluation measures in the context of probability assessment. We show that these scoring rules designed for probability assessment provide inappropriate incentives if used for quantile assessment. We investigate the properties of a linear family of scoring rules that are intended specifically for quantile assessment (including the assessment of multiple quantiles) and can be related to a realistic decision-making problem. These rules provide proper incentives for quantile assessment and yield higher expected scores for distributions that are more informative in the sense of having less dispersion. We discuss the special case of interval forecasts and a generalization involving transformations, and we briefly mention other possible extensions.},
  keywords = {/unread},
  file = {C:\Users\fabio\Zotero\storage\S6MM45XN\Jose and Winkler - 2009 - Evaluating Quantile Assessments.pdf}
}

@article{Merkle_etal_2016,
  title = {Item Response Models of Probability Judgments: {{Application}} to a Geopolitical Forecasting Tournament},
  shorttitle = {Item Response Models of Probability Judgments},
  author = {Merkle, Edgar C. and Steyvers, Mark and Mellers, Barbara and Tetlock, Philip E.},
  date = {2016},
  journaltitle = {Decision},
  volume = {3},
  number = {1},
  pages = {1--19},
  publisher = {Educational Publishing Foundation},
  location = {US},
  issn = {2325-9973},
  doi = {10.1037/dec0000032},
  abstract = {In this article, we develop and study methods for evaluating forecasters and forecasting questions in dynamic environments. These methods, based on item response models, are useful in situations where items vary in difficulty, and we wish to evaluate forecasters based on the difficulty of the items that they forecasted correctly. In addition, the methods are useful in situations where we need to compare forecasters who make predictions at different points in time or for different items. We first extend traditional models to handle subjective probabilities, and we then apply a specific model to geopolitical forecasts. We evaluate the model’s ability to accommodate the data, compare the model’s estimates of forecaster ability to estimates of forecaster ability based on scoring rules, and externally validate the model’s item estimates. We also highlight some shortcomings of the traditional models and discuss some further extensions. The analyses illustrate the models’ potential for widespread use in forecasting and subjective probability evaluation. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Item Response Theory,Methodology,Models,Prediction,Probability,Probability Judgment},
  file = {C:\Users\fabio\Zotero\storage\JXVXZC4W\Merkle et al. - 2016 - Item response models of probability judgments App.pdf}
}

@online{Merkle_etal_2024,
  title = {Identifying Good Forecasters via Adaptive Cognitive Tests},
  author = {Merkle, Edgar C. and Petrov, Nikolay and Zhu, Sophie Ma and Karger, Ezra and Tetlock, Philip E. and Himmelstein, Mark},
  date = {2024-11-17},
  eprint = {2411.11126},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2411.11126},
  url = {http://arxiv.org/abs/2411.11126},
  urldate = {2024-12-16},
  abstract = {Assessing forecasting proficiency is a time-intensive activity, often requiring us to wait months or years before we know whether or not the reported forecasts were good. In this study, we develop adaptive cognitive tests that predict forecasting proficiency without the need to wait for forecast outcomes. Our procedures provide information about which cognitive tests to administer to each individual, as well as how many cognitive tests to administer. Using item response models, we identify and tailor cognitive tests to assess forecasters of different skill levels, aiming to optimize accuracy and efficiency. We show how the procedures can select highly-informative cognitive tests from a larger battery of tests, reducing the time taken to administer the tests. We use a second, independent dataset to show that the selected tests yield scores that are highly related to forecasting proficiency. This approach enables real-time, adaptive testing, providing immediate insights into forecasting talent in practical contexts.},
  pubstate = {prepublished},
  keywords = {Statistics - Applications},
  file = {C\:\\Users\\fabio\\Zotero\\storage\\LUUKJV4V\\Merkle et al. - 2024 - Identifying good forecasters via adaptive cognitiv.pdf;C\:\\Users\\fabio\\Zotero\\storage\\85CKI8LP\\2411.html}
}


@online{Zhu_etal_2024,
  title = {The {{Psychometric Properties}} of {{Probability}} and {{Quantile Forecasts}}},
  author = {Zhu, Sophie Ma and Budescu, David and Petrov, Nikolay and Karger, Ezra and Himmelstein, Mark},
  date = {2024-11-19},
  eprinttype = {OSF},
  doi = {10.31234/osf.io/2m4ya},
  url = {https://osf.io/2m4ya},
  urldate = {2024-12-16},
  abstract = {Forecasting tournaments are a well established method for assessing human forecasting skills. Most forecasting tournaments are based on a format where participants estimate the probabilities of discrete events. For predictions of continuous values, the possible range of outcome values is divided into mutually exclusive bins covering the entire outcome distribution so that probabilities for each bin can be elicited. An alternative approach involves directly eliciting forecasts about quantiles of the continuous quantity. Using both simulated data and data from 1,147 participants who completed five surveys focused on forecasting tasks in a longitudinal study, we compared the psychometric properties of quantile and probability elicitation methods. In the simulation, we demonstrated that items in the quantile format recovered parameters that defined forecasters’ latent forecasting skill in fewer items than the probability format, and identified several idiosyncrasies in accuracy scores for the probability format that drive these differences. In the empirical analyses, we elicited forecasts about a set of 36 forecasting questions in both formats: quantile forecasts at five fixed probability values (5\%, 25\%, 50\%, 75\%, 95\%) and probability forecasts for five pre-determined item-specific bins. Consistent with the simulated results, our findings revealed that forecasts in the quantile format showed considerably stronger internal consistency, achieving a suitable reliability level with fewer items. When cross-validating how well individual forecasters’ accuracy on in-sample questions predicted their performance in out-of-sample questions, the variability in the accuracy of quantile forecasts was more statistically explainable. Despite its desirable properties, errors and signs of comprehension difficulties were more frequently observed in the quantile format. Further research is needed to refine elicitations that optimize the effectiveness of quantile-based forecasting judgments.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {Elicitation Methods,Forecasting,Superforecasters},
  file = {C:\Users\fabio\Zotero\storage\DPN9HZV9\Zhu et al. - 2024 - The Psychometric Properties of Probability and Qua.pdf}
}


@article{carpenter2017stan,
title={Stan: A probabilistic programming language},
author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
journal={Journal of statistical software},
volume={76},
number={1},
year={2017},
publisher={Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA (United States)}
}



@article{pymc2023,
  title = {{{PyMC}}: A Modern, and Comprehensive Probabilistic Programming Framework in {{Python}}},
  shorttitle = {{{PyMC}}},
  author = {Abril-Pla, Oriol and Andreani, Virgile and Carroll, Colin and Dong, Larry and Fonnesbeck, Christopher J. and Kochurov, Maxim and Kumar, Ravin and Lao, Junpeng and Luhmann, Christian C. and Martin, Osvaldo A. and Osthege, Michael and Vieira, Ricardo and Wiecki, Thomas and Zinkov, Robert},
  date = {2023-09-01},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  volume = {9},
  pages = {e1516},
  publisher = {PeerJ Inc.},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.1516},
  url = {https://peerj.com/articles/cs-1516},
  urldate = {2025-06-07},
  abstract = {PyMC is a probabilistic programming library for Python that provides tools for constructing and fitting Bayesian models. It offers an intuitive, readable syntax that is close to the natural syntax statisticians use to describe models. PyMC leverages the symbolic computation library PyTensor, allowing it to be compiled into a variety of computational backends, such as C, JAX, and Numba, which in turn offer access to different computational architectures including CPU, GPU, and TPU. Being a general modeling framework, PyMC supports a variety of models including generalized hierarchical linear regression and classification, time series, ordinary differential equations (ODEs), and non-parametric models such as Gaussian processes (GPs). We demonstrate PyMC’s versatility and ease of use with examples spanning a range of common statistical models. Additionally, we discuss the positive role of PyMC in the development of the open-source ecosystem for probabilistic programming.},
  langid = {english},
  file = {C:\Users\fabio\Zotero\storage\XJSDLAD9\Abril-Pla et al. - 2023 - PyMC a modern, and comprehensive probabilistic programming framework in Python.pdf}
}
